{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aea9ca-c8ae-46c5-9846-2065f0ddf193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "# Function to keep search agent positions within bounds\n",
    "def boundary(position, lb, ub):\n",
    "    return np.clip(position, lb, ub)\n",
    "\n",
    "# Define the cache outside the function\n",
    "cache = {}\n",
    "\n",
    "# Fitness function for XGBoost\n",
    "def fitness_xgboost(solution, X1, y1, Xt, yt, w1=1, w2=1):\n",
    "    key = tuple(solution)  # Cache key\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "\n",
    "    n_estimators = int(solution[0])\n",
    "    max_depth = int(solution[1])\n",
    "    learning_rate = solution[2]\n",
    "    model = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n",
    "                         objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "    model.fit(X1, y1)\n",
    "    predictions = model.predict(Xt)\n",
    "    mse = mean_squared_error(yt, predictions)\n",
    "    r2 = r2_score(yt, predictions)\n",
    "\n",
    "    fitness_value = w1 * mse - w2 * r2  # Weighted MSE and R2 for optimization\n",
    "    cache[key] = fitness_value  # Store result in cache\n",
    "    return fitness_value\n",
    "\n",
    "# Fitness function for RandomForest\n",
    "def fitness_random_forest(solution, X1, y1, Xt, yt):\n",
    "    n_estimators = int(solution[0])\n",
    "    max_depth = int(solution[1])\n",
    "    min_samples_split = int(solution[2])\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
    "    model.fit(X1, y1)\n",
    "    predictions = model.predict(Xt)\n",
    "    mse = mean_squared_error(yt, predictions)\n",
    "    return mse  # Optimize MSE\n",
    "\n",
    "# Harris Hawks Optimization Algorithm (HHO) for XGBoost\n",
    "def hho_for_xgboost(SearchAgents_no, Max_iter, X1, y1, Xt, yt, w1=1, w2=1):\n",
    "    dim = 3  # Dimension of the optimization problem (n_estimators, max_depth, learning_rate)\n",
    "    Leader_pos = np.zeros(dim)  # Leader position (best solution)\n",
    "    Leader_score = float('inf')  # Leader's fitness score\n",
    "\n",
    "    # Boundaries of the search space for XGBoost\n",
    "    lb = np.array([50, 1, 0.01])  # Lower bounds\n",
    "    ub = np.array([500, 50, 0.5])  # Upper bounds\n",
    "\n",
    "    # Initialize the location of the search agents\n",
    "    Positions = np.zeros((SearchAgents_no, dim))\n",
    "    for i in range(SearchAgents_no):\n",
    "        Positions[i, :] = np.array([ \n",
    "            random.randint(int(lb[0]), int(ub[0])), \n",
    "            random.randint(int(lb[1]), int(ub[1])), \n",
    "            random.uniform(lb[2], ub[2]) \n",
    "        ])\n",
    "\n",
    "    Convergence_curve = np.zeros(Max_iter)\n",
    "\n",
    "    for t in range(Max_iter):\n",
    "        E1 = 2 * (1 - t / Max_iter)  # Energy parameter decreases linearly\n",
    "\n",
    "        for i in range(SearchAgents_no):\n",
    "            # Calculate fitness of the current agent\n",
    "            fit = fitness_xgboost(Positions[i, :], X1, y1, Xt, yt, w1, w2)\n",
    "\n",
    "            # Update the leader if the current agent is better\n",
    "            if fit < Leader_score:\n",
    "                Leader_score = fit\n",
    "                Leader_pos = Positions[i, :].copy()\n",
    "\n",
    "        for i in range(SearchAgents_no):\n",
    "            E0 = 2 * random.random() - 1  # Random energy\n",
    "            E = E1 * E0  # Current energy of the prey\n",
    "            Q = random.random()  # Jumping strength\n",
    "            J = 2 * (1 - random.random())  # Randomization parameter\n",
    "\n",
    "            if abs(E) >= 1:  # Exploration phase\n",
    "                rand_idx = random.randint(0, SearchAgents_no - 1)\n",
    "                X_rand = Positions[rand_idx, :]\n",
    "                Positions[i, :] = X_rand - random.random() * abs(X_rand - 2 * random.random() * Positions[i, :])\n",
    "            else:  # Exploitation phase\n",
    "                if Q < 0.5:  # Soft besiege\n",
    "                    Positions[i, :] = Leader_pos - E * abs(J * Leader_pos - Positions[i, :])\n",
    "                else:  # Hard besiege\n",
    "                    Positions[i, :] = (Leader_pos - Positions[i, :]) - E * abs(J * Leader_pos - Positions[i, :])\n",
    "\n",
    "            # Ensure agents stay within the search space\n",
    "            Positions[i, :] = boundary(Positions[i, :], lb, ub)\n",
    "\n",
    "            # Calculate fitness of the updated agent\n",
    "            fit = fitness_xgboost(Positions[i, :], X1, y1, Xt, yt, w1, w2)\n",
    "\n",
    "            # Update the leader if the updated agent is better\n",
    "            if fit < Leader_score:\n",
    "                Leader_score = fit\n",
    "                Leader_pos = Positions[i, :].copy()\n",
    "\n",
    "        Convergence_curve[t] = Leader_score  # Record the best fitness\n",
    "\n",
    "    return Leader_pos, Convergence_curve\n",
    "\n",
    "# Harris Hawks Optimization Algorithm (HHO) for Random Forest\n",
    "def hho_for_random_forest(SearchAgents_no, Max_iter, X1, y1, Xt, yt):\n",
    "    dim = 3  # Dimension of the optimization problem (n_estimators, max_depth, min_samples_split)\n",
    "    Leader_pos = np.zeros(dim)  # Leader position (best solution)\n",
    "    Leader_score = float('inf')  # Leader's fitness score\n",
    "\n",
    "    # Boundaries of the search space for RandomForest\n",
    "    lb = np.array([50, 1, 1])  # Lower bounds for n_estimators, max_depth, min_samples_split\n",
    "    ub = np.array([500, 50, 50])  # Upper bounds\n",
    "\n",
    "    # Initialize the location of the search agents\n",
    "    Positions = np.zeros((SearchAgents_no, dim))\n",
    "    for i in range(SearchAgents_no):\n",
    "        Positions[i, :] = np.array([ \n",
    "            random.randint(lb[0], ub[0]), \n",
    "            random.randint(lb[1], ub[1]), \n",
    "            random.randint(lb[2], ub[2]) \n",
    "        ])\n",
    "\n",
    "    Convergence_curve = np.zeros(Max_iter)\n",
    "\n",
    "    for t in range(Max_iter):\n",
    "        E1 = 2 * (1 - t / Max_iter)  # Energy parameter decreases linearly\n",
    "\n",
    "        for i in range(SearchAgents_no):\n",
    "            # Calculate fitness of the current agent\n",
    "            fit = fitness_random_forest(Positions[i, :], X1, y1, Xt, yt)\n",
    "\n",
    "            # Update the leader if the current agent is better\n",
    "            if fit < Leader_score:\n",
    "                Leader_score = fit\n",
    "                Leader_pos = Positions[i, :].copy()\n",
    "\n",
    "        for i in range(SearchAgents_no):\n",
    "            E0 = 2 * random.random() - 1  # Random energy\n",
    "            E = E1 * E0  # Current energy of the prey\n",
    "            Q = random.random()  # Jumping strength\n",
    "            J = 2 * (1 - random.random())  # Randomization parameter\n",
    "\n",
    "            if abs(E) >= 1:  # Exploration phase\n",
    "                rand_idx = random.randint(0, SearchAgents_no - 1)\n",
    "                X_rand = Positions[rand_idx, :]\n",
    "                Positions[i, :] = X_rand - random.random() * abs(X_rand - 2 * random.random() * Positions[i, :])\n",
    "            else:  # Exploitation phase\n",
    "                if Q < 0.5:  # Soft besiege\n",
    "                    Positions[i, :] = Leader_pos - E * abs(J * Leader_pos - Positions[i, :])\n",
    "                else:  # Hard besiege\n",
    "                    Positions[i, :] = (Leader_pos - Positions[i, :]) - E * abs(J * Leader_pos - Positions[i, :])\n",
    "\n",
    "            # Ensure agents stay within the search space\n",
    "            Positions[i, :] = boundary(Positions[i, :], lb, ub)\n",
    "\n",
    "            # Calculate fitness of the updated agent\n",
    "            fit = fitness_random_forest(Positions[i, :], X1, y1, Xt, yt)\n",
    "\n",
    "            # Update the leader if the updated agent is better\n",
    "            if fit < Leader_score:\n",
    "                Leader_score = fit\n",
    "                Leader_pos = Positions[i, :].copy()\n",
    "\n",
    "        Convergence_curve[t] = Leader_score  # Record the best fitness\n",
    "\n",
    "    return Leader_pos, Convergence_curve\n",
    "\n",
    "\n",
    "def get_best_params_from_leader(Leader_pos, model_type):\n",
    "    if model_type == 'xgboost':\n",
    "        return {\n",
    "            'n_estimators': int(Leader_pos[0]),\n",
    "            'max_depth': int(Leader_pos[1]),\n",
    "            'learning_rate': Leader_pos[2]\n",
    "        }\n",
    "    elif model_type == 'random_forest':\n",
    "        return {\n",
    "            'n_estimators': int(Leader_pos[0]),\n",
    "            'max_depth': int(Leader_pos[1]),\n",
    "            'min_samples_split': int(Leader_pos[2])\n",
    "        }\n",
    "\n",
    "# 固定随机种子，保证每次运行的结果一致\n",
    "np.random.seed(42)\n",
    "# 加载数据\n",
    "df = pd.read_excel('soil thickness 环境因子.xlsx')\n",
    "\n",
    "# 分离输入和输出\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# 定义两组特征列\n",
    "factors_group_1 = ['NDVI', 'TWI', 'ASP', 'LU', 'P', 'SI', 'C']   # 第一组特征\n",
    "factors_group_2 = ['NDVI', 'TWI', 'ASP', 'LU', 'SSP', 'SSSI', 'C'] # 第二组特征\n",
    "\n",
    "# 初始化XGBoost和RandomForest的结果列表\n",
    "xgb_results_group_1 = []\n",
    "xgb_results_group_2 = []\n",
    "rf_results_group_1 = []\n",
    "rf_results_group_2 = []\n",
    "\n",
    "\n",
    "# 设置迭代次数和群体大小\n",
    "SearchAgents_no = 50\n",
    "Max_iter = 20\n",
    "# Perform 100 training and testing of HHO-RF models\n",
    "\n",
    "for iteration in range(100):\n",
    "    # 每次迭代都重新划分数据集，确保两个模型使用相同的训练集和测试集\n",
    "    train_data, test_data, train_target, test_target = train_test_split(X, y, train_size=0.7, random_state=iteration)\n",
    "    # Perform HHO optimization\n",
    "    \n",
    "\n",
    "\n",
    "    # 提取不同特征组的数据\n",
    "    train_data_group_1 = train_data[factors_group_1]\n",
    "    test_data_group_1 = test_data[factors_group_1]\n",
    "    \n",
    "    train_data_group_2 = train_data[factors_group_2]\n",
    "    test_data_group_2 = test_data[factors_group_2]\n",
    "\n",
    "   \n",
    "    # 运行HHO优化算法并获取最佳参数\n",
    "    # 对第一组特征进行XGBoost优化\n",
    "    xgb_params_group_1_optimized, _ = hho_for_xgboost(SearchAgents_no, Max_iter, train_data_group_1, train_target, test_data_group_1, test_target)\n",
    "    xgb_params_group_1_optimized = get_best_params_from_leader(xgb_params_group_1_optimized, 'xgboost')\n",
    "    \n",
    "    # 对第二组特征进行XGBoost优化\n",
    "    xgb_params_group_2_optimized, _ = hho_for_xgboost(SearchAgents_no, Max_iter, train_data_group_2, train_target, test_data_group_2, test_target)\n",
    "    xgb_params_group_2_optimized = get_best_params_from_leader(xgb_params_group_2_optimized, 'xgboost')\n",
    "    \n",
    "    # 对第一组特征进行RandomForest优化\n",
    "    rf_params_group_1_optimized, _ = hho_for_random_forest(SearchAgents_no, Max_iter, train_data_group_1, train_target, test_data_group_1, test_target)\n",
    "    rf_params_group_1_optimized = get_best_params_from_leader(rf_params_group_1_optimized, 'random_forest')\n",
    "    \n",
    "    # 对第二组特征进行RandomForest优化\n",
    "    rf_params_group_2_optimized, _ = hho_for_random_forest(SearchAgents_no, Max_iter, train_data_group_2, train_target, test_data_group_2, test_target)\n",
    "    rf_params_group_2_optimized = get_best_params_from_leader(rf_params_group_2_optimized, 'random_forest')\n",
    "\n",
    "    \n",
    "    # 训练 XGBoost（第一组特征）\n",
    "    model_xgb_group_1 = XGBRegressor(**xgb_params_group_1_optimized)\n",
    "    model_xgb_group_1.fit(train_data_group_1, train_target)\n",
    "    \n",
    "    # 训练 XGBoost（第二组特征）\n",
    "    model_xgb_group_2 = XGBRegressor(**xgb_params_group_2_optimized)\n",
    "    model_xgb_group_2.fit(train_data_group_2, train_target)\n",
    "    \n",
    "    # 训练 RandomForest（第一组特征）\n",
    "    model_rf_group_1 = RandomForestRegressor(**rf_params_group_1_optimized)\n",
    "    model_rf_group_1.fit(train_data_group_1, train_target)\n",
    "\n",
    "    # 训练 RandomForest（第二组特征）\n",
    "    model_rf_group_2 = RandomForestRegressor(**rf_params_group_2_optimized)\n",
    "    model_rf_group_2.fit(train_data_group_2, train_target)\n",
    "\n",
    "    # 预测和性能评估（RandomForest）\n",
    "    train_pred_rf_group_1 = model_rf_group_1.predict(train_data_group_1)\n",
    "    test_pred_rf_group_1 = model_rf_group_1.predict(test_data_group_1)\n",
    "\n",
    "    train_pred_rf_group_2 = model_rf_group_2.predict(train_data_group_2)\n",
    "    test_pred_rf_group_2 = model_rf_group_2.predict(test_data_group_2)\n",
    "\n",
    "    # 预测和性能评估（XGBoost）\n",
    "    train_pred_xgb_group_1 = model_xgb_group_1.predict(train_data_group_1)\n",
    "    test_pred_xgb_group_1 = model_xgb_group_1.predict(test_data_group_1)\n",
    "\n",
    "    train_pred_xgb_group_2 = model_xgb_group_2.predict(train_data_group_2)\n",
    "    test_pred_xgb_group_2 = model_xgb_group_2.predict(test_data_group_2)\n",
    "\n",
    "    # 计算MSE和RMSE\n",
    "    train_mse_rf_group_1 = mean_squared_error(train_target, train_pred_rf_group_1)\n",
    "    test_mse_rf_group_1 = mean_squared_error(test_target, test_pred_rf_group_1)\n",
    "\n",
    "    train_mse_rf_group_2 = mean_squared_error(train_target, train_pred_rf_group_2)\n",
    "    test_mse_rf_group_2 = mean_squared_error(test_target, test_pred_rf_group_2)\n",
    "\n",
    "    train_mse_xgb_group_1 = mean_squared_error(train_target, train_pred_xgb_group_1)\n",
    "    test_mse_xgb_group_1 = mean_squared_error(test_target, test_pred_xgb_group_1)\n",
    "\n",
    "    train_mse_xgb_group_2 = mean_squared_error(train_target, train_pred_xgb_group_2)\n",
    "    test_mse_xgb_group_2 = mean_squared_error(test_target, test_pred_xgb_group_2)\n",
    "\n",
    "    # 计算RMSE\n",
    "    train_rmse_rf_group_1 = np.sqrt(train_mse_rf_group_1)\n",
    "    test_rmse_rf_group_1 = np.sqrt(test_mse_rf_group_1)\n",
    "\n",
    "    train_rmse_rf_group_2 = np.sqrt(train_mse_rf_group_2)\n",
    "    test_rmse_rf_group_2 = np.sqrt(test_mse_rf_group_2)\n",
    "\n",
    "    train_rmse_xgb_group_1 = np.sqrt(train_mse_xgb_group_1)\n",
    "    test_rmse_xgb_group_1 = np.sqrt(test_mse_xgb_group_1)\n",
    "\n",
    "    train_rmse_xgb_group_2 = np.sqrt(train_mse_xgb_group_2)\n",
    "    test_rmse_xgb_group_2 = np.sqrt(test_mse_xgb_group_2)\n",
    "\n",
    "    # 计算R2\n",
    "    train_r2_rf_group_1 = r2_score(train_target, train_pred_rf_group_1)\n",
    "    test_r2_rf_group_1 = r2_score(test_target, test_pred_rf_group_1)\n",
    "\n",
    "    train_r2_rf_group_2 = r2_score(train_target, train_pred_rf_group_2)\n",
    "    test_r2_rf_group_2 = r2_score(test_target, test_pred_rf_group_2)\n",
    "\n",
    "    train_r2_xgb_group_1 = r2_score(train_target, train_pred_xgb_group_1)\n",
    "    test_r2_xgb_group_1 = r2_score(test_target, test_pred_xgb_group_1)\n",
    "\n",
    "    train_r2_xgb_group_2 = r2_score(train_target, train_pred_xgb_group_2)\n",
    "    test_r2_xgb_group_2 = r2_score(test_target, test_pred_xgb_group_2)\n",
    "    # 输出每次迭代的结果\n",
    "\n",
    "\n",
    "    # 将每次迭代的结果保存到列表\n",
    "    rf_results_group_1.append({\n",
    "        'iteration': iteration + 1,\n",
    "        'train_mse': train_mse_rf_group_1, 'test_mse': test_mse_rf_group_1,\n",
    "        'train_rmse': train_rmse_rf_group_1, 'test_rmse': test_rmse_rf_group_1,\n",
    "        'train_r2': train_r2_rf_group_1, 'test_r2': test_r2_rf_group_1,\n",
    "        'n_estimators': rf_params_group_1_optimized['n_estimators'],\n",
    "        'max_depth': rf_params_group_1_optimized['max_depth'],\n",
    "        'min_samples_split': rf_params_group_1_optimized['min_samples_split']\n",
    "    })\n",
    "    rf_results_group_2.append({\n",
    "        'iteration': iteration + 1,\n",
    "        'train_mse': train_mse_rf_group_2, 'test_mse': test_mse_rf_group_2,\n",
    "        'train_rmse': train_rmse_rf_group_2, 'test_rmse': test_rmse_rf_group_2,\n",
    "        'train_r2': train_r2_rf_group_2, 'test_r2': test_r2_rf_group_2,\n",
    "        'n_estimators': rf_params_group_2_optimized['n_estimators'],\n",
    "        'max_depth': rf_params_group_2_optimized['max_depth'],\n",
    "        'min_samples_split': rf_params_group_2_optimized['min_samples_split']\n",
    "    })\n",
    "\n",
    "    xgb_results_group_1.append({\n",
    "        'iteration': iteration + 1,\n",
    "        'train_mse': train_mse_xgb_group_1, 'test_mse': test_mse_xgb_group_1,\n",
    "        'train_rmse': train_rmse_xgb_group_1, 'test_rmse': test_rmse_xgb_group_1,\n",
    "        'train_r2': train_r2_xgb_group_1, 'test_r2': test_r2_xgb_group_1,\n",
    "        'n_estimators': xgb_params_group_1_optimized['n_estimators'],\n",
    "        'max_depth': xgb_params_group_1_optimized['max_depth'],\n",
    "        'learning_rate': xgb_params_group_1_optimized['learning_rate']\n",
    "    })\n",
    "    xgb_results_group_2.append({\n",
    "        'iteration': iteration + 1,\n",
    "        'train_mse': train_mse_xgb_group_2, 'test_mse': test_mse_xgb_group_2,\n",
    "        'train_rmse': train_rmse_xgb_group_2, 'test_rmse': test_rmse_xgb_group_2,\n",
    "        'train_r2': train_r2_xgb_group_2, 'test_r2': test_r2_xgb_group_2,\n",
    "        'n_estimators': xgb_params_group_2_optimized['n_estimators'],\n",
    "        'max_depth': xgb_params_group_2_optimized['max_depth'],\n",
    "        'learning_rate': xgb_params_group_2_optimized['learning_rate']\n",
    "    })\n",
    "        # Print performance metrics for each iteration\n",
    "    # 打印每次迭代的结果\n",
    "    print(f\"Iteration {iteration + 1} Results:\")\n",
    "    print(\"Random Forest Group 1:\", rf_results_group_1[-1])\n",
    "    print(\"Random Forest Group 2:\", rf_results_group_2[-1])\n",
    "    print(\"XGBoost Group 1:\", xgb_results_group_1[-1])\n",
    "    print(\"XGBoost Group 2:\", xgb_results_group_2[-1])\n",
    "\n",
    "# 将结果转换为DataFrame\n",
    "# 将结果转化为DataFrame进行输出\n",
    "xgb_results_group_1_df = pd.DataFrame(xgb_results_group_1)\n",
    "xgb_results_group_2_df = pd.DataFrame(xgb_results_group_2)\n",
    "rf_results_group_1_df = pd.DataFrame(rf_results_group_1)\n",
    "rf_results_group_2_df = pd.DataFrame(rf_results_group_2)\n",
    "\n",
    "# 分别保存为四个独立的Excel文件\n",
    "xgb_results_group_1_df.to_excel('8.xlsx', index=False)\n",
    "xgb_results_group_2_df.to_excel('88.xlsx', index=False)\n",
    "rf_results_group_1_df.to_excel('888.xlsx', index=False)\n",
    "rf_results_group_2_df.to_excel('8888.xlsx', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d07220-61ff-41bb-baae-4e5fa7a71a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf151a7-7b03-49d6-b1fc-21ea4fa3bb89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
